#!/usr/bin/env python
"""
SmolVLM-256M-Instruct GGUF æœåŠ¡å¯åŠ¨è„šæœ¬
ä½¿ç”¨ llama-cpp-python çš„ OpenAI å…¼å®¹ API
"""

import os
import sys
import subprocess
from pathlib import Path

# æ£€æŸ¥ llama-cpp-python æ˜¯å¦å®‰è£…
try:
    import llama_cpp
    print("âœ… llama-cpp-python å·²å®‰è£…")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å…ˆè¿è¡Œ: pip install -r requirements.txt")
    sys.exit(1)

# é…ç½®
MODEL_NAME = "SmolVLM-256M-Instruct-Q8_0.gguf"
MODEL_PATH = Path(__file__).parent / MODEL_NAME
MMPROJ_NAME = "mmproj-SmolVLM-256M-Instruct-f16.gguf"
MMPROJ_PATH = Path(__file__).parent / MMPROJ_NAME
HOST = "127.0.0.1"
PORT = 8888

def find_model():
    """æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶"""
    # ç›´æ¥ä½¿ç”¨å½“å‰ç›®å½•çš„æ¨¡å‹æ–‡ä»¶
    if MODEL_PATH.exists():
        return str(MODEL_PATH)
    
    print(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {MODEL_PATH}")
    return None

def find_mmproj():
    """æŸ¥æ‰¾è§†è§‰ç¼–ç å™¨æ–‡ä»¶"""
    if MMPROJ_PATH.exists():
        return str(MMPROJ_PATH)
    
    print(f"âš ï¸  æœªæ‰¾åˆ°è§†è§‰ç¼–ç å™¨æ–‡ä»¶: {MMPROJ_PATH}")
    return None

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ SmolVLM-256M-Instruct GGUF æœåŠ¡å¯åŠ¨")
    print("=" * 60)
    print()
    
    model_path = find_model()
    
    if not model_path:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        print(f"ğŸ“ æœŸæœ›è·¯å¾„: {MODEL_PATH}")
        sys.exit(1)
    
    mmproj_path = find_mmproj()
    
    print(f"âœ… æ‰¾åˆ°æ¨¡å‹: {model_path}")
    if mmproj_path:
        print(f"âœ… æ‰¾åˆ°è§†è§‰ç¼–ç å™¨: {mmproj_path}")
    print(f"ğŸ“ ç›‘å¬åœ°å€: http://{HOST}:{PORT}")
    print(f"ğŸ“š API æ–‡æ¡£: http://{HOST}:{PORT}/docs")
    print(f"ğŸ¥ å¥åº·æ£€æŸ¥: http://{HOST}:{PORT}/v1/models")
    print()
    print("â¹ï¸  æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print()
    
    # å¯åŠ¨ llama-cpp-python server
    cmd = [
        sys.executable,
        "-m",
        "llama_cpp.server",
        "--model",
        model_path,
        "--host",
        HOST,
        "--port",
        str(PORT),
        "--n_gpu_layers",
        "-1",  # ä½¿ç”¨ GPU åŠ é€Ÿ
        "--n_threads",
        "4",
        "--n_batch",
        "512",
        "--chat_format",
        "llava-1-5",  # ä½¿ç”¨ LLaVA æ ¼å¼æ”¯æŒè§†è§‰
    ]
    
    # å¦‚æœæ‰¾åˆ°è§†è§‰ç¼–ç å™¨ï¼Œæ·»åŠ åˆ°å‘½ä»¤
    if mmproj_path:
        cmd.extend(["--clip_model_path", mmproj_path])
    
    print(f"ğŸ”„ å¯åŠ¨å‘½ä»¤: {' '.join(cmd)}")
    print()
    
    try:
        subprocess.run(cmd, check=True)
    except KeyboardInterrupt:
        print("\nğŸ”´ æœåŠ¡å·²åœæ­¢")
    except subprocess.CalledProcessError as e:
        print(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()

